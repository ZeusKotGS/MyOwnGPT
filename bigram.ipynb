{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "#mapping from characters to integers\n",
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "sentence = \"hii there\"\n",
    "print(encode(sentence))\n",
    "print(decode(encode(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
      "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
      "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
      "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
      "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
      "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
      "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
      "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
      "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
      "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
      "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
      "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
      "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
      "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
      "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
      "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
      "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
      "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
      "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
      "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
      "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
      "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
      "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
      "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
      "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
      "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
      "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
      "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
      "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
      "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
      "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
      "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
      "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
      "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
      "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
      "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
      "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
      "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
      "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
      "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
      "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
      "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
      "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
      "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "# encoding the entire text dataset\n",
    "# storing in a torch.Tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val split\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets: \n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets: ')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (b, T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "\n",
    "        return idx\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# expected log based on negative log likelihood is -ln(1/65)\n",
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.692410945892334\n",
      "4.664144515991211\n",
      "4.765714645385742\n",
      "4.70655632019043\n",
      "4.5956573486328125\n",
      "4.7101240158081055\n",
      "4.713661193847656\n",
      "4.686909198760986\n",
      "4.700076103210449\n",
      "4.718283653259277\n",
      "4.715603351593018\n",
      "4.684308052062988\n",
      "4.745601177215576\n",
      "4.735717296600342\n",
      "4.666238784790039\n",
      "4.58615255355835\n",
      "4.714625835418701\n",
      "4.671982765197754\n",
      "4.715047359466553\n",
      "4.744891166687012\n",
      "4.630162715911865\n",
      "4.707578182220459\n",
      "4.670665740966797\n",
      "4.582583427429199\n",
      "4.739546298980713\n",
      "4.674807071685791\n",
      "4.805595874786377\n",
      "4.749917507171631\n",
      "4.691989421844482\n",
      "4.604404926300049\n",
      "4.721841335296631\n",
      "4.741592884063721\n",
      "4.6099629402160645\n",
      "4.662769794464111\n",
      "4.730099678039551\n",
      "4.738433361053467\n",
      "4.688235282897949\n",
      "4.639987945556641\n",
      "4.736632823944092\n",
      "4.709773540496826\n",
      "4.736939430236816\n",
      "4.69184684753418\n",
      "4.719646453857422\n",
      "4.752516746520996\n",
      "4.570086479187012\n",
      "4.643786907196045\n",
      "4.699163913726807\n",
      "4.806960105895996\n",
      "4.572142601013184\n",
      "4.717066287994385\n",
      "4.509502410888672\n",
      "4.603540897369385\n",
      "4.6649675369262695\n",
      "4.712099075317383\n",
      "4.736577033996582\n",
      "4.812878131866455\n",
      "4.596436977386475\n",
      "4.702690601348877\n",
      "4.711158752441406\n",
      "4.636075019836426\n",
      "4.706498146057129\n",
      "4.706602573394775\n",
      "4.776336193084717\n",
      "4.538954257965088\n",
      "4.595245838165283\n",
      "4.648927688598633\n",
      "4.668923854827881\n",
      "4.5940351486206055\n",
      "4.757757186889648\n",
      "4.683036804199219\n",
      "4.627960205078125\n",
      "4.809708118438721\n",
      "4.720103740692139\n",
      "4.69432258605957\n",
      "4.593808650970459\n",
      "4.627923488616943\n",
      "4.628483772277832\n",
      "4.594723224639893\n",
      "4.667132377624512\n",
      "4.563992500305176\n",
      "4.598272323608398\n",
      "4.718154430389404\n",
      "4.686451435089111\n",
      "4.564647197723389\n",
      "4.6638665199279785\n",
      "4.6551690101623535\n",
      "4.503818511962891\n",
      "4.662378311157227\n",
      "4.597469329833984\n",
      "4.553940773010254\n",
      "4.6458845138549805\n",
      "4.658196926116943\n",
      "4.652643203735352\n",
      "4.572150230407715\n",
      "4.654421806335449\n",
      "4.505650997161865\n",
      "4.6306376457214355\n",
      "4.7071919441223145\n",
      "4.6614508628845215\n",
      "4.65630578994751\n",
      "4.621085166931152\n",
      "4.699936389923096\n",
      "4.604835510253906\n",
      "4.626178741455078\n",
      "4.598467826843262\n",
      "4.562093257904053\n",
      "4.61775541305542\n",
      "4.545349597930908\n",
      "4.5832133293151855\n",
      "4.523378849029541\n",
      "4.568169593811035\n",
      "4.605664253234863\n",
      "4.625260829925537\n",
      "4.56585168838501\n",
      "4.675260066986084\n",
      "4.607123851776123\n",
      "4.707728385925293\n",
      "4.575418949127197\n",
      "4.564972877502441\n",
      "4.588582515716553\n",
      "4.592921733856201\n",
      "4.5604448318481445\n",
      "4.7858452796936035\n",
      "4.569169521331787\n",
      "4.5731329917907715\n",
      "4.601418495178223\n",
      "4.576107978820801\n",
      "4.60406494140625\n",
      "4.595185279846191\n",
      "4.515230655670166\n",
      "4.476064205169678\n",
      "4.521181583404541\n",
      "4.655724048614502\n",
      "4.520238399505615\n",
      "4.546555042266846\n",
      "4.582465171813965\n",
      "4.651482105255127\n",
      "4.5329389572143555\n",
      "4.552127361297607\n",
      "4.610964298248291\n",
      "4.720569610595703\n",
      "4.532519817352295\n",
      "4.590734004974365\n",
      "4.659023284912109\n",
      "4.569576740264893\n",
      "4.501828193664551\n",
      "4.534751892089844\n",
      "4.5555877685546875\n",
      "4.4378557205200195\n",
      "4.584807872772217\n",
      "4.493810176849365\n",
      "4.496149063110352\n",
      "4.61268424987793\n",
      "4.541433811187744\n",
      "4.462303161621094\n",
      "4.494714736938477\n",
      "4.5445098876953125\n",
      "4.591465473175049\n",
      "4.559953212738037\n",
      "4.522037029266357\n",
      "4.604924201965332\n",
      "4.505509376525879\n",
      "4.600823402404785\n",
      "4.534110069274902\n",
      "4.466233730316162\n",
      "4.4869561195373535\n",
      "4.499792098999023\n",
      "4.558073043823242\n",
      "4.611894607543945\n",
      "4.532322406768799\n",
      "4.494655132293701\n",
      "4.501111030578613\n",
      "4.487987518310547\n",
      "4.500681400299072\n",
      "4.447858810424805\n",
      "4.551181793212891\n",
      "4.5280585289001465\n",
      "4.566673278808594\n",
      "4.546204566955566\n",
      "4.588383197784424\n",
      "4.5528364181518555\n",
      "4.483713626861572\n",
      "4.496376037597656\n",
      "4.405621528625488\n",
      "4.482258319854736\n",
      "4.484110355377197\n",
      "4.4490227699279785\n",
      "4.338376522064209\n",
      "4.460762977600098\n",
      "4.561094284057617\n",
      "4.521778106689453\n",
      "4.64401912689209\n",
      "4.514046669006348\n",
      "4.4592413902282715\n",
      "4.4573655128479\n",
      "4.652929782867432\n",
      "4.569197177886963\n",
      "4.549731254577637\n",
      "4.605109691619873\n",
      "4.531442642211914\n",
      "4.549462795257568\n",
      "4.4532389640808105\n",
      "4.42667293548584\n",
      "4.428462505340576\n",
      "4.5073981285095215\n",
      "4.5997443199157715\n",
      "4.428247451782227\n",
      "4.516406059265137\n",
      "4.531017780303955\n",
      "4.524899005889893\n",
      "4.43112325668335\n",
      "4.413572311401367\n",
      "4.460278511047363\n",
      "4.394894123077393\n",
      "4.440585136413574\n",
      "4.515986442565918\n",
      "4.501888751983643\n",
      "4.540366172790527\n",
      "4.476080417633057\n",
      "4.480386257171631\n",
      "4.420383930206299\n",
      "4.483820915222168\n",
      "4.512678146362305\n",
      "4.560887336730957\n",
      "4.491804599761963\n",
      "4.480713844299316\n",
      "4.423666954040527\n",
      "4.433342456817627\n",
      "4.417287826538086\n",
      "4.414680480957031\n",
      "4.418458461761475\n",
      "4.457849025726318\n",
      "4.501733779907227\n",
      "4.55715799331665\n",
      "4.42041015625\n",
      "4.500718116760254\n",
      "4.4469828605651855\n",
      "4.404678821563721\n",
      "4.448772430419922\n",
      "4.4832024574279785\n",
      "4.401884078979492\n",
      "4.445903301239014\n",
      "4.52253532409668\n",
      "4.410618305206299\n",
      "4.45168924331665\n",
      "4.405672073364258\n",
      "4.359124660491943\n",
      "4.418424606323242\n",
      "4.447394371032715\n",
      "4.4999918937683105\n",
      "4.495026111602783\n",
      "4.441216945648193\n",
      "4.3983259201049805\n",
      "4.399979591369629\n",
      "4.396550178527832\n",
      "4.515158176422119\n",
      "4.436650276184082\n",
      "4.254152774810791\n",
      "4.4689860343933105\n",
      "4.4714837074279785\n",
      "4.331572532653809\n",
      "4.43186616897583\n",
      "4.361902713775635\n",
      "4.399670124053955\n",
      "4.441038608551025\n",
      "4.391478061676025\n",
      "4.487184524536133\n",
      "4.294976234436035\n",
      "4.41543436050415\n",
      "4.420255661010742\n",
      "4.351208209991455\n",
      "4.358051300048828\n",
      "4.529325008392334\n",
      "4.34090518951416\n",
      "4.488083839416504\n",
      "4.442676067352295\n",
      "4.422273635864258\n",
      "4.372884273529053\n",
      "4.420283317565918\n",
      "4.343837261199951\n",
      "4.380279064178467\n",
      "4.460439682006836\n",
      "4.4696574211120605\n",
      "4.4250054359436035\n",
      "4.369536876678467\n",
      "4.380033016204834\n",
      "4.365662097930908\n",
      "4.339999675750732\n",
      "4.48419713973999\n",
      "4.555813789367676\n",
      "4.4380035400390625\n",
      "4.477532386779785\n",
      "4.378237724304199\n",
      "4.336991310119629\n",
      "4.448875427246094\n",
      "4.361961841583252\n",
      "4.365668296813965\n",
      "4.469748020172119\n",
      "4.392701625823975\n",
      "4.334584712982178\n",
      "4.345611572265625\n",
      "4.306689739227295\n",
      "4.438986301422119\n",
      "4.454972267150879\n",
      "4.497692584991455\n",
      "4.392993927001953\n",
      "4.369708061218262\n",
      "4.354208469390869\n",
      "4.376923084259033\n",
      "4.388126850128174\n",
      "4.346914291381836\n",
      "4.454780101776123\n",
      "4.369599342346191\n",
      "4.400058746337891\n",
      "4.321812152862549\n",
      "4.430188179016113\n",
      "4.363119125366211\n",
      "4.258288860321045\n",
      "4.324169635772705\n",
      "4.358462333679199\n",
      "4.358053207397461\n",
      "4.346726417541504\n",
      "4.4252777099609375\n",
      "4.3356428146362305\n",
      "4.356608867645264\n",
      "4.268974304199219\n",
      "4.34548282623291\n",
      "4.391635894775391\n",
      "4.350566387176514\n",
      "4.366872310638428\n",
      "4.406307220458984\n",
      "4.266676425933838\n",
      "4.258195877075195\n",
      "4.293352127075195\n",
      "4.3209686279296875\n",
      "4.332687854766846\n",
      "4.352051258087158\n",
      "4.382238864898682\n",
      "4.383143901824951\n",
      "4.244317531585693\n",
      "4.3100972175598145\n",
      "4.301767826080322\n",
      "4.462956428527832\n",
      "4.325232028961182\n",
      "4.341349124908447\n",
      "4.296145439147949\n",
      "4.235404014587402\n",
      "4.250915050506592\n",
      "4.308598518371582\n",
      "4.3766632080078125\n",
      "4.237582683563232\n",
      "4.350257396697998\n",
      "4.228846549987793\n",
      "4.305774211883545\n",
      "4.354750633239746\n",
      "4.467280864715576\n",
      "4.318342685699463\n",
      "4.338566780090332\n",
      "4.289865970611572\n",
      "4.2690815925598145\n",
      "4.180973529815674\n",
      "4.387163162231445\n",
      "4.275718688964844\n",
      "4.193756580352783\n",
      "4.348846435546875\n",
      "4.311291694641113\n",
      "4.2306294441223145\n",
      "4.389198303222656\n",
      "4.380277633666992\n",
      "4.376615524291992\n",
      "4.3441596031188965\n",
      "4.27800989151001\n",
      "4.265659332275391\n",
      "4.27650260925293\n",
      "4.285951614379883\n",
      "4.260744094848633\n",
      "4.254425525665283\n",
      "4.387601852416992\n",
      "4.293213367462158\n",
      "4.275619029998779\n",
      "4.264163017272949\n",
      "4.214735507965088\n",
      "4.323124885559082\n",
      "4.272207260131836\n",
      "4.2730560302734375\n",
      "4.332861423492432\n",
      "4.228231906890869\n",
      "4.272346496582031\n",
      "4.331362724304199\n",
      "4.377825736999512\n",
      "4.194937705993652\n",
      "4.29219913482666\n",
      "4.230798721313477\n",
      "4.251380920410156\n",
      "4.296419620513916\n",
      "4.252346992492676\n",
      "4.2831315994262695\n",
      "4.3855204582214355\n",
      "4.1769537925720215\n",
      "4.227319717407227\n",
      "4.25573205947876\n",
      "4.238279819488525\n",
      "4.283568382263184\n",
      "4.299123287200928\n",
      "4.25726318359375\n",
      "4.330580711364746\n",
      "4.313518524169922\n",
      "4.335666179656982\n",
      "4.248019695281982\n",
      "4.212539196014404\n",
      "4.390359878540039\n",
      "4.26472806930542\n",
      "4.235802173614502\n",
      "4.21953821182251\n",
      "4.3849310874938965\n",
      "4.191295146942139\n",
      "4.29976224899292\n",
      "4.263235092163086\n",
      "4.209293365478516\n",
      "4.223510265350342\n",
      "4.197415351867676\n",
      "4.352291584014893\n",
      "4.358164310455322\n",
      "4.247671604156494\n",
      "4.329915523529053\n",
      "4.220633506774902\n",
      "4.240585803985596\n",
      "4.314285755157471\n",
      "4.260376453399658\n",
      "4.259975433349609\n",
      "4.2319464683532715\n",
      "4.184201240539551\n",
      "4.277885913848877\n",
      "4.279393196105957\n",
      "4.180922508239746\n",
      "4.270190238952637\n",
      "4.253904819488525\n",
      "4.179213047027588\n",
      "4.2253098487854\n",
      "4.279184341430664\n",
      "4.301351547241211\n",
      "4.229222297668457\n",
      "4.343070030212402\n",
      "4.276855945587158\n",
      "4.191098690032959\n",
      "4.293546199798584\n",
      "4.201022148132324\n",
      "4.206254959106445\n",
      "4.195919990539551\n",
      "4.2168192863464355\n",
      "4.221164703369141\n",
      "4.163954257965088\n",
      "4.355230331420898\n",
      "4.183740139007568\n",
      "4.180233955383301\n",
      "4.350985050201416\n",
      "4.179173946380615\n",
      "4.266575336456299\n",
      "4.145872592926025\n",
      "4.3596954345703125\n",
      "4.116206169128418\n",
      "4.278004169464111\n",
      "4.263311386108398\n",
      "4.26434850692749\n",
      "4.247138977050781\n",
      "4.2241597175598145\n",
      "4.163585662841797\n",
      "4.185435771942139\n",
      "4.255035877227783\n",
      "4.19374418258667\n",
      "4.256316661834717\n",
      "4.281538963317871\n",
      "4.171205043792725\n",
      "4.182270526885986\n",
      "4.194500923156738\n",
      "4.2833991050720215\n",
      "4.299307823181152\n",
      "4.241708755493164\n",
      "4.283669948577881\n",
      "4.143308639526367\n",
      "4.15635347366333\n",
      "4.232965469360352\n",
      "4.241590976715088\n",
      "4.036279201507568\n",
      "4.157522201538086\n",
      "4.231443405151367\n",
      "4.147864818572998\n",
      "4.156970024108887\n",
      "4.223931789398193\n",
      "4.105999946594238\n",
      "4.140515327453613\n",
      "4.220064163208008\n",
      "4.173741817474365\n",
      "4.072698593139648\n",
      "4.158931255340576\n",
      "4.1450676918029785\n",
      "4.184323787689209\n",
      "4.103063583374023\n",
      "4.092406272888184\n",
      "4.1035356521606445\n",
      "4.214480876922607\n",
      "4.084356307983398\n",
      "4.133087158203125\n",
      "4.223405838012695\n",
      "4.220446586608887\n",
      "4.17624568939209\n",
      "4.128087043762207\n",
      "4.1905412673950195\n",
      "4.2345123291015625\n",
      "4.164053440093994\n",
      "4.095093250274658\n",
      "4.139039039611816\n",
      "4.137739181518555\n",
      "4.16702127456665\n",
      "4.160588264465332\n",
      "4.111112594604492\n",
      "4.019512176513672\n",
      "4.21013879776001\n",
      "4.071273326873779\n",
      "4.1922783851623535\n",
      "4.240005016326904\n",
      "4.245697021484375\n",
      "4.147686958312988\n",
      "4.2053656578063965\n",
      "4.214274883270264\n",
      "4.0432515144348145\n",
      "4.149839401245117\n",
      "4.167660713195801\n",
      "4.083763122558594\n",
      "4.21851110458374\n",
      "4.211829662322998\n",
      "4.033554553985596\n",
      "4.161428928375244\n",
      "4.165452003479004\n",
      "4.050341606140137\n",
      "4.156692981719971\n",
      "4.160954475402832\n",
      "4.093878269195557\n",
      "4.132650375366211\n",
      "4.1404008865356445\n",
      "4.130082130432129\n",
      "4.175171375274658\n",
      "4.112301826477051\n",
      "4.073479175567627\n",
      "4.167318820953369\n",
      "4.1959919929504395\n",
      "4.078147888183594\n",
      "4.069385528564453\n",
      "4.063492774963379\n",
      "4.158783435821533\n",
      "4.046174049377441\n",
      "4.117081642150879\n",
      "4.245076656341553\n",
      "4.109870433807373\n",
      "4.118143081665039\n",
      "4.16467809677124\n",
      "4.108386039733887\n",
      "4.165163040161133\n",
      "4.048225402832031\n",
      "4.135279655456543\n",
      "4.158371925354004\n",
      "4.126312732696533\n",
      "3.964298725128174\n",
      "4.15919303894043\n",
      "4.119665622711182\n",
      "4.238160133361816\n",
      "4.114436626434326\n",
      "4.085212230682373\n",
      "4.048520088195801\n",
      "4.12908411026001\n",
      "4.118659496307373\n",
      "4.099023818969727\n",
      "4.183162689208984\n",
      "4.058305740356445\n",
      "4.143056392669678\n",
      "4.114629745483398\n",
      "4.0972514152526855\n",
      "4.099020481109619\n",
      "4.109713554382324\n",
      "4.0857157707214355\n",
      "4.125316143035889\n",
      "4.175577640533447\n",
      "4.063840389251709\n",
      "4.108927249908447\n",
      "4.104191303253174\n",
      "4.119007110595703\n",
      "4.09041166305542\n",
      "4.067192077636719\n",
      "4.102412223815918\n",
      "4.113567352294922\n",
      "4.192253589630127\n",
      "4.100230693817139\n",
      "4.120125770568848\n",
      "4.143221378326416\n",
      "4.08186674118042\n",
      "4.087782382965088\n",
      "4.074817657470703\n",
      "4.0610480308532715\n",
      "4.183136463165283\n",
      "4.0359954833984375\n",
      "4.124096870422363\n",
      "4.125417709350586\n",
      "4.087498188018799\n",
      "4.018767356872559\n",
      "4.11121940612793\n",
      "4.014806270599365\n",
      "4.001387596130371\n",
      "3.9844272136688232\n",
      "4.148004055023193\n",
      "4.025826454162598\n",
      "4.057997703552246\n",
      "3.9568910598754883\n",
      "4.0929460525512695\n",
      "4.072330474853516\n",
      "4.03060245513916\n",
      "4.048306941986084\n",
      "4.133228778839111\n",
      "4.038398265838623\n",
      "4.12543249130249\n",
      "4.072927474975586\n",
      "4.045656204223633\n",
      "4.1224045753479\n",
      "4.121188163757324\n",
      "4.098836898803711\n",
      "4.161966323852539\n",
      "4.04520845413208\n",
      "4.056055068969727\n",
      "3.9817774295806885\n",
      "4.114030361175537\n",
      "4.055196762084961\n",
      "4.181158065795898\n",
      "4.015369415283203\n",
      "4.070910453796387\n",
      "4.0655999183654785\n",
      "4.0205183029174805\n",
      "4.009420871734619\n",
      "4.009321689605713\n",
      "4.121938705444336\n",
      "4.040282726287842\n",
      "4.006514072418213\n",
      "3.9769175052642822\n",
      "3.9687490463256836\n",
      "4.053492546081543\n",
      "4.024524211883545\n",
      "4.1228790283203125\n",
      "3.9823174476623535\n",
      "3.9732022285461426\n",
      "4.015324592590332\n",
      "4.034710884094238\n",
      "4.076888561248779\n",
      "4.0141777992248535\n",
      "4.0707106590271\n",
      "4.029590606689453\n",
      "4.0675764083862305\n",
      "3.958583116531372\n",
      "4.049327850341797\n",
      "3.9634346961975098\n",
      "4.109421730041504\n",
      "4.038191795349121\n",
      "4.125021934509277\n",
      "4.031307220458984\n",
      "4.070112705230713\n",
      "3.9946374893188477\n",
      "4.0985188484191895\n",
      "3.9959027767181396\n",
      "4.035952568054199\n",
      "4.017228603363037\n",
      "4.024997234344482\n",
      "3.9557952880859375\n",
      "4.03995418548584\n",
      "4.067239761352539\n",
      "4.015563011169434\n",
      "4.054080486297607\n",
      "4.0563883781433105\n",
      "3.9339613914489746\n",
      "3.991907835006714\n",
      "3.9665045738220215\n",
      "4.003228664398193\n",
      "4.0467000007629395\n",
      "4.060911178588867\n",
      "3.9072952270507812\n",
      "4.017506122589111\n",
      "4.071604251861572\n",
      "4.083905220031738\n",
      "4.046928405761719\n",
      "4.020440101623535\n",
      "3.9562582969665527\n",
      "3.857187509536743\n",
      "3.9379942417144775\n",
      "4.030976295471191\n",
      "4.080989837646484\n",
      "3.94681715965271\n",
      "4.019579887390137\n",
      "3.9735379219055176\n",
      "4.019238471984863\n",
      "4.019179344177246\n",
      "3.9712741374969482\n",
      "4.026883602142334\n",
      "4.021551609039307\n",
      "4.090793132781982\n",
      "3.9863951206207275\n",
      "4.023050308227539\n",
      "3.958601474761963\n",
      "3.948958158493042\n",
      "3.9391560554504395\n",
      "3.979315996170044\n",
      "4.01404333114624\n",
      "3.987943410873413\n",
      "4.037786483764648\n",
      "3.9132330417633057\n",
      "3.93660569190979\n",
      "3.9861817359924316\n",
      "3.9920737743377686\n",
      "3.9355592727661133\n",
      "4.01882791519165\n",
      "3.9929792881011963\n",
      "3.959758758544922\n",
      "3.9396843910217285\n",
      "4.044256687164307\n",
      "4.004631996154785\n",
      "3.996853828430176\n",
      "4.005602836608887\n",
      "4.0114874839782715\n",
      "3.909653663635254\n",
      "3.9997363090515137\n",
      "4.052602291107178\n",
      "3.982553720474243\n",
      "3.9164369106292725\n",
      "3.942601203918457\n",
      "3.9261810779571533\n",
      "3.895392894744873\n",
      "3.9324474334716797\n",
      "3.9205896854400635\n",
      "3.862194061279297\n",
      "3.914445400238037\n",
      "3.8829867839813232\n",
      "3.958883285522461\n",
      "3.985666036605835\n",
      "3.899744987487793\n",
      "3.9055404663085938\n",
      "3.9618642330169678\n",
      "3.889052391052246\n",
      "3.9750149250030518\n",
      "3.955091953277588\n",
      "3.9291818141937256\n",
      "4.087095737457275\n",
      "3.9071226119995117\n",
      "3.909717082977295\n",
      "3.9263739585876465\n",
      "3.936264991760254\n",
      "3.9365663528442383\n",
      "3.938066005706787\n",
      "3.970029830932617\n",
      "3.9482243061065674\n",
      "3.8840718269348145\n",
      "3.952329635620117\n",
      "3.8996760845184326\n",
      "3.868910789489746\n",
      "3.926823377609253\n",
      "3.8954102993011475\n",
      "4.0050368309021\n",
      "3.949986696243286\n",
      "3.946974277496338\n",
      "3.8552606105804443\n",
      "4.010585784912109\n",
      "4.076684951782227\n",
      "3.9837636947631836\n",
      "3.980055570602417\n",
      "4.009552478790283\n",
      "3.981788158416748\n",
      "3.9590961933135986\n",
      "3.8400533199310303\n",
      "3.88386607170105\n",
      "3.8691532611846924\n",
      "3.783965826034546\n",
      "3.9049429893493652\n",
      "3.9803521633148193\n",
      "3.9815800189971924\n",
      "3.8576600551605225\n",
      "3.8969061374664307\n",
      "3.9168310165405273\n",
      "3.8810322284698486\n",
      "3.8791961669921875\n",
      "3.9240477085113525\n",
      "3.853982448577881\n",
      "3.9570164680480957\n",
      "3.8988609313964844\n",
      "3.906435489654541\n",
      "3.8281397819519043\n",
      "3.8748936653137207\n",
      "3.7957098484039307\n",
      "3.889361619949341\n",
      "3.8909873962402344\n",
      "3.9083378314971924\n",
      "3.9673891067504883\n",
      "3.7797915935516357\n",
      "3.875593900680542\n",
      "3.9253997802734375\n",
      "3.9070682525634766\n",
      "3.947261333465576\n",
      "3.9517807960510254\n",
      "3.9063897132873535\n",
      "3.9117953777313232\n",
      "3.870689630508423\n",
      "3.8529112339019775\n",
      "3.899077892303467\n",
      "3.8730580806732178\n",
      "3.8374412059783936\n",
      "3.9627771377563477\n",
      "3.884561061859131\n",
      "3.8707284927368164\n",
      "3.812702178955078\n",
      "3.9962551593780518\n",
      "3.9128105640411377\n",
      "3.903803825378418\n",
      "3.901273012161255\n",
      "3.8220882415771484\n",
      "3.8226213455200195\n",
      "3.8852691650390625\n",
      "3.820089817047119\n",
      "3.8163671493530273\n",
      "3.7907116413116455\n",
      "3.8253843784332275\n",
      "3.8396363258361816\n",
      "3.8457422256469727\n",
      "3.8769731521606445\n",
      "3.8439455032348633\n",
      "3.876339912414551\n",
      "3.908766746520996\n",
      "3.8244807720184326\n",
      "3.840985059738159\n",
      "3.755624294281006\n",
      "3.857574939727783\n",
      "3.9703409671783447\n",
      "3.698105812072754\n",
      "3.7928614616394043\n",
      "3.8883657455444336\n",
      "3.9447708129882812\n",
      "3.814781665802002\n",
      "3.8951289653778076\n",
      "3.802658796310425\n",
      "3.8546929359436035\n",
      "3.90655255317688\n",
      "3.7954976558685303\n",
      "3.802588939666748\n",
      "3.848461389541626\n",
      "3.8054921627044678\n",
      "3.831833839416504\n",
      "3.858421564102173\n",
      "3.7204763889312744\n",
      "3.9306366443634033\n",
      "3.8283700942993164\n",
      "3.828134775161743\n",
      "3.854487895965576\n",
      "3.8038671016693115\n",
      "3.739840030670166\n",
      "3.8414194583892822\n",
      "3.840139627456665\n",
      "3.858626365661621\n",
      "3.8682198524475098\n",
      "3.7725791931152344\n",
      "3.740438222885132\n",
      "3.853250503540039\n",
      "3.909426212310791\n",
      "3.827120304107666\n",
      "3.9142749309539795\n",
      "3.8566269874572754\n",
      "3.836930990219116\n",
      "3.8507373332977295\n",
      "3.7641050815582275\n",
      "3.8466036319732666\n",
      "3.8550658226013184\n",
      "3.814547538757324\n",
      "3.858736515045166\n",
      "3.8355484008789062\n",
      "3.820634126663208\n",
      "3.8446009159088135\n",
      "3.8565011024475098\n",
      "3.793048858642578\n",
      "3.803140640258789\n",
      "3.877279758453369\n",
      "3.915501356124878\n",
      "3.737924337387085\n",
      "3.7830746173858643\n",
      "3.773717164993286\n",
      "3.769728660583496\n",
      "3.76853084564209\n",
      "3.774484395980835\n",
      "3.908461093902588\n",
      "3.785860300064087\n",
      "3.840094804763794\n",
      "3.9154369831085205\n",
      "3.7070677280426025\n",
      "3.876446485519409\n",
      "3.8132681846618652\n",
      "3.7787420749664307\n",
      "3.808058023452759\n",
      "3.7111711502075195\n",
      "3.7674307823181152\n",
      "3.886543035507202\n",
      "3.837888717651367\n",
      "3.7464873790740967\n",
      "3.8434603214263916\n",
      "3.8652989864349365\n",
      "3.9370501041412354\n",
      "3.7554266452789307\n",
      "3.806457996368408\n",
      "3.7218282222747803\n",
      "3.841688394546509\n",
      "3.838592529296875\n",
      "3.7659902572631836\n",
      "3.8026270866394043\n",
      "3.730456829071045\n",
      "3.7594165802001953\n",
      "3.868187189102173\n",
      "3.8208365440368652\n",
      "3.8494954109191895\n",
      "3.6943864822387695\n",
      "3.825169324874878\n",
      "3.802319049835205\n",
      "3.7781949043273926\n",
      "3.8065850734710693\n",
      "3.795403003692627\n",
      "3.826564073562622\n",
      "3.7271037101745605\n",
      "3.769099473953247\n",
      "3.814549684524536\n",
      "3.7501776218414307\n",
      "3.8552398681640625\n",
      "3.7635231018066406\n",
      "3.757671594619751\n",
      "3.7829813957214355\n",
      "3.856001377105713\n",
      "3.786350965499878\n",
      "3.7992660999298096\n",
      "3.853888511657715\n",
      "3.7279489040374756\n",
      "3.77148699760437\n",
      "3.6774191856384277\n",
      "3.7062582969665527\n",
      "3.8540217876434326\n",
      "3.714585065841675\n",
      "3.75681209564209\n",
      "3.8052268028259277\n",
      "3.750839948654175\n",
      "3.7266221046447754\n",
      "3.8073298931121826\n",
      "3.621952772140503\n",
      "3.7910683155059814\n",
      "3.783512592315674\n",
      "3.916217565536499\n",
      "3.8640291690826416\n",
      "3.75536847114563\n",
      "3.7300283908843994\n",
      "3.797163963317871\n",
      "3.7928595542907715\n",
      "3.7810394763946533\n",
      "3.8614728450775146\n",
      "3.801198720932007\n",
      "3.831483840942383\n",
      "3.7052838802337646\n",
      "3.786813735961914\n",
      "3.731647253036499\n",
      "3.7757680416107178\n",
      "3.7474052906036377\n",
      "3.761415481567383\n",
      "3.806239128112793\n",
      "3.7878215312957764\n",
      "3.8432199954986572\n",
      "3.7990047931671143\n",
      "3.7246756553649902\n",
      "3.711846113204956\n",
      "3.7594316005706787\n",
      "3.83084774017334\n",
      "3.6742985248565674\n",
      "3.684211015701294\n",
      "3.766167163848877\n",
      "3.725360155105591\n",
      "3.788503408432007\n",
      "3.7539544105529785\n",
      "3.74045467376709\n",
      "3.6661875247955322\n",
      "3.8223159313201904\n",
      "3.707447052001953\n",
      "3.723768949508667\n",
      "3.678398847579956\n",
      "3.639932155609131\n",
      "3.683429479598999\n",
      "3.703873634338379\n",
      "3.670771360397339\n",
      "3.747714042663574\n",
      "3.8158376216888428\n",
      "3.765026569366455\n",
      "3.719977617263794\n",
      "3.709388017654419\n",
      "3.685523271560669\n",
      "3.697873115539551\n",
      "3.727125883102417\n",
      "3.804948091506958\n",
      "3.704137086868286\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(1000):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    #evaluatle the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wh;;Sq.f ustNzknc\n",
      "kwgOj$dhPWr,SV?hsusiKpgXXUh;Apmem d?hESXI.i;TrJgkiF-oKbXCAA -botrngFCHAUQkn$\n",
      "\n",
      "pn$w-gHoi?wtd!\n",
      "LLULIfSK'bAw :M.ZtOptXEQcL?hfaofqbPd?OnonQQJMap$aypupIBYGUsZaI'ottllo..k$W$Akp?yl?ajKlzY!lx&QQLW? t,bXFkyhl-dmVsHeckhRl,jSClgjuk:3Iv\n",
      "?OqlrV;!Plxfzgy;;\n",
      "'mRjuBQ&xk!$\n",
      "h\n",
      "SiruDJgKuDny,S$ERf.?GSV-ivvKcOvi-nQGX&q-YQbm dEM?px;Akr-IESq--wIWId\n",
      "RFgXTpDUgM:CK$I!uo'IBT -\n",
      "j?wfy fFr.&fiqtRS.ZttxGh' a!ogrn$zoZqbocL&yIffBDWNUboscuQqo.Fls,?,M?eZxHx?p?EV.mJiHqHnxT  bQpa;P fawiF$-QbWv&f:CVDCBfano,b?$Esev.?Y fYHat g:p a!jKH;A.vyDnuCnR:o. gL$kgRClxl,jUgCDX?d.ZPOnypwf CJuprep$H-wnp;:rdhiN-t$N.jEt-kq-lvW\n",
      "YtrIcfFq, fZTF-nNLv&'-gRy;.OWSKFMI&Pm ziq-e?wNXERK'ssuxte''\n",
      "\n",
      "xQBO f?lvirlyxowJ'd\n",
      "YQq Fo,.Deq&xira;V.\n",
      "JUjsrk:DgStLaPZO 3V&C,\n",
      "hivickexHzmylg-dlHA:CKobHEHm cmaNAr;p, woXeZDWnDWeQXY\n",
      "iPYGoGSYw wO C KmGretNEqT:hlvvyvCKoCehiWcPk$eVto'TEhdx;UCJTM:hicL&L&onsl;DncWwK;KIVJWel.mE!3d.\n",
      "YWYWTHOK:ChiMbQklbLznhObWe:C!!!BU'nuibOF$ENCVqStrxJhord&oCHAGCXEb?!JnBoicLEHotRotrSKWhJibo\n",
      "aAcHUMviHrus,SAHIUWPCj3TsDq, thattGSq?u\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(torch.zeros((1, 1), dtype=torch.long), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss():\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'eval']:\n",
    "        losses = torch.zeros(10)\n",
    "        for k in range(10):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 2.5384, val loss 2.5031\n",
      "step 10: train loss 2.5704, val loss 2.5569\n",
      "step 20: train loss 2.5723, val loss 2.5896\n",
      "step 30: train loss 2.5471, val loss 2.5316\n",
      "step 40: train loss 2.5597, val loss 2.5647\n",
      "step 50: train loss 2.5352, val loss 2.5311\n",
      "step 60: train loss 2.5637, val loss 2.5637\n",
      "step 70: train loss 2.5890, val loss 2.5749\n",
      "step 80: train loss 2.5511, val loss 2.5774\n",
      "step 90: train loss 2.5472, val loss 2.5851\n",
      "step 100: train loss 2.5229, val loss 2.5761\n",
      "step 110: train loss 2.5543, val loss 2.5705\n",
      "step 120: train loss 2.5607, val loss 2.6202\n",
      "step 130: train loss 2.6115, val loss 2.6101\n",
      "step 140: train loss 2.5604, val loss 2.5915\n",
      "step 150: train loss 2.5772, val loss 2.5907\n",
      "step 160: train loss 2.5480, val loss 2.5896\n",
      "step 170: train loss 2.5916, val loss 2.5181\n",
      "step 180: train loss 2.5852, val loss 2.5462\n",
      "step 190: train loss 2.5397, val loss 2.5853\n",
      "step 200: train loss 2.5241, val loss 2.5675\n",
      "step 210: train loss 2.5385, val loss 2.5536\n",
      "step 220: train loss 2.5478, val loss 2.5539\n",
      "step 230: train loss 2.5470, val loss 2.5620\n",
      "step 240: train loss 2.5806, val loss 2.5655\n",
      "step 250: train loss 2.5495, val loss 2.5645\n",
      "step 260: train loss 2.5488, val loss 2.5398\n",
      "step 270: train loss 2.5374, val loss 2.5578\n",
      "step 280: train loss 2.5405, val loss 2.5308\n",
      "step 290: train loss 2.5034, val loss 2.5763\n",
      "step 300: train loss 2.5749, val loss 2.5578\n",
      "step 310: train loss 2.5731, val loss 2.5289\n",
      "step 320: train loss 2.5477, val loss 2.5355\n",
      "step 330: train loss 2.5405, val loss 2.5418\n",
      "step 340: train loss 2.5159, val loss 2.5595\n",
      "step 350: train loss 2.5463, val loss 2.5414\n",
      "step 360: train loss 2.5581, val loss 2.5117\n",
      "step 370: train loss 2.5109, val loss 2.5471\n",
      "step 380: train loss 2.5393, val loss 2.5823\n",
      "step 390: train loss 2.5486, val loss 2.5834\n",
      "step 400: train loss 2.5519, val loss 2.5841\n",
      "step 410: train loss 2.5081, val loss 2.5433\n",
      "step 420: train loss 2.5566, val loss 2.5702\n",
      "step 430: train loss 2.5149, val loss 2.5253\n",
      "step 440: train loss 2.5708, val loss 2.5499\n",
      "step 450: train loss 2.5155, val loss 2.4969\n",
      "step 460: train loss 2.5639, val loss 2.5780\n",
      "step 470: train loss 2.5435, val loss 2.5632\n",
      "step 480: train loss 2.5476, val loss 2.5476\n",
      "step 490: train loss 2.5234, val loss 2.5833\n",
      "step 500: train loss 2.5069, val loss 2.5461\n",
      "step 510: train loss 2.5066, val loss 2.5647\n",
      "step 520: train loss 2.5463, val loss 2.5097\n",
      "step 530: train loss 2.5306, val loss 2.5362\n",
      "step 540: train loss 2.5689, val loss 2.5647\n",
      "step 550: train loss 2.5237, val loss 2.5666\n",
      "step 560: train loss 2.5118, val loss 2.5619\n",
      "step 570: train loss 2.5320, val loss 2.5080\n",
      "step 580: train loss 2.5299, val loss 2.5071\n",
      "step 590: train loss 2.5702, val loss 2.5602\n",
      "step 600: train loss 2.5335, val loss 2.5430\n",
      "step 610: train loss 2.5305, val loss 2.5154\n",
      "step 620: train loss 2.5281, val loss 2.5312\n",
      "step 630: train loss 2.5141, val loss 2.5398\n",
      "step 640: train loss 2.4935, val loss 2.5337\n",
      "step 650: train loss 2.5144, val loss 2.5434\n",
      "step 660: train loss 2.5405, val loss 2.5006\n",
      "step 670: train loss 2.5362, val loss 2.5278\n",
      "step 680: train loss 2.4641, val loss 2.5473\n",
      "step 690: train loss 2.5605, val loss 2.5476\n",
      "step 700: train loss 2.5653, val loss 2.4995\n",
      "step 710: train loss 2.5464, val loss 2.5488\n",
      "step 720: train loss 2.5557, val loss 2.5259\n",
      "step 730: train loss 2.5217, val loss 2.5064\n",
      "step 740: train loss 2.5176, val loss 2.5563\n",
      "step 750: train loss 2.5131, val loss 2.5038\n",
      "step 760: train loss 2.5324, val loss 2.5609\n",
      "step 770: train loss 2.5434, val loss 2.5595\n",
      "step 780: train loss 2.5045, val loss 2.5448\n",
      "step 790: train loss 2.5040, val loss 2.5578\n",
      "step 800: train loss 2.4916, val loss 2.5448\n",
      "step 810: train loss 2.5351, val loss 2.5314\n",
      "step 820: train loss 2.5210, val loss 2.5196\n",
      "step 830: train loss 2.5106, val loss 2.5444\n",
      "step 840: train loss 2.5106, val loss 2.5241\n",
      "step 850: train loss 2.5247, val loss 2.5267\n",
      "step 860: train loss 2.5300, val loss 2.5377\n",
      "step 870: train loss 2.5170, val loss 2.5214\n",
      "step 880: train loss 2.4955, val loss 2.5314\n",
      "step 890: train loss 2.5259, val loss 2.5478\n",
      "step 900: train loss 2.5392, val loss 2.4885\n",
      "step 910: train loss 2.4942, val loss 2.5344\n",
      "step 920: train loss 2.5114, val loss 2.5658\n",
      "step 930: train loss 2.5457, val loss 2.5548\n",
      "step 940: train loss 2.5505, val loss 2.5489\n",
      "step 950: train loss 2.5093, val loss 2.5661\n",
      "step 960: train loss 2.5278, val loss 2.4880\n",
      "step 970: train loss 2.5600, val loss 2.5358\n",
      "step 980: train loss 2.5003, val loss 2.5778\n",
      "step 990: train loss 2.5023, val loss 2.5387\n",
      "\n",
      "Wivesou.\n",
      "Hxit thiners CKENIAUxFindiOLA:\n",
      "DYXef d an f\n",
      "ASAR: f SBisis k shoureemaigard\n",
      "Taincilas; r o ism R3Q'd ouston'd o cig m ce DOLOLLAy ar h.\n",
      "\n",
      "Inoo lve mr GAby u t f malaPle ougYGJTore aru.\n",
      "\n",
      "A:\n",
      "\n",
      "KI'd t quabbeep, IA bful s, ir giZ&ve hof II wit inesuplainougCa e, thawherthe:wimasperd y pand me bet ponn styelker he n :\n",
      "And ad the astouerARIORI:\n",
      "ENIORK:wee.\n",
      "Ux is wofo zom:\n",
      "Asoull oveeClldd.\n",
      "Dakf t\n",
      "HMy, torstwin'st nTothen gea;$&y tot.\n",
      "Myo fit$HAMy teney t teis woultan, rount!\n",
      "HOL&Cleenge BROMllo\n"
     ]
    }
   ],
   "source": [
    "for iter in range(1000):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % 10 == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['eval']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    #evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
